{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What drives the price of a car?\n",
    "\n",
    "![](images/kurt.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**OVERVIEW**\n",
    "\n",
    "In this application, you will explore a dataset from kaggle. The original dataset contained information on 3 million used cars. The provided dataset contains information on 426K cars to ensure speed of processing.  Your goal is to understand what factors make a car more or less expensive.  As a result of your analysis, you should provide clear recommendations to your client -- a used car dealership -- as to what consumers value in a used car."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CRISP-DM Framework\n",
    "\n",
    "<center>\n",
    "    <img src = images/crisp.png width = 50%/>\n",
    "</center>\n",
    "\n",
    "\n",
    "To frame the task, throughout our practical applications we will refer back to a standard process in industry for data projects called CRISP-DM.  This process provides a framework for working through a data problem.  Your first step in this application will be to read through a brief overview of CRISP-DM [here](https://mo-pcco.s3.us-east-1.amazonaws.com/BH-PCMLAI/module_11/readings_starter.zip).  After reading the overview, answer the questions below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Business Understanding\n",
    "\n",
    "### Background:\n",
    "Used car sales dealership wants to know what drives the price of the car so that they can appropriately price the car to gain buy or sell the used cars.\n",
    "### Business Objectives: \n",
    "Identify what are the features or attributes that drives the price of the car. Set the price for the car for sale based on findings.\n",
    "### Business Success Criteria: \n",
    "Set the car price appropriately either for buying or selling to gain maximum profitability\n",
    "### Access Situation: \n",
    "We need to identify the dataset that contains several features of any given car such as make, model, odometer reading, condition, color, location etc.\n",
    "### Risks: \n",
    "There may be a possibility of the data is not accurate or insufficient or not useful\n",
    "### Costs/Benefits: \n",
    "The cost to run analysis to give the price range for any given car so that the delarship can sell appropriately.\n",
    "### Data Mining/Success goals: \n",
    "Identify data, collect the data elements, and samples for various sources. Here the data set is given.\n",
    "### Project Plan: \n",
    "Prepare a project plan to collect data, resources, data analysis,  modelling, deployment and monitoring.\n",
    "### Tools: \n",
    "Linear Regression, Python, Jupyter notebook, and several algorithms, validations etc.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Understanding\n",
    "\n",
    "### Collect Initial Data:\n",
    "Run several campaigns to collect the data. Here we got the dataset, so we are not doing anything.\n",
    "### Describe Data:\n",
    "The data has 426880 data points with 18 features with several data types.\n",
    "### Explore Data: \n",
    "Explore the data with techniques to identify what features are important using several plots, detecting outliers, treat missing values,data type changes, data cleansing, and null checks etc.\n",
    "### Verify Data Quality: \n",
    "Inspect the data for quality, consistency, range, and length etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-30T03:43:44.684162Z",
     "start_time": "2023-08-30T03:43:44.682015Z"
    }
   },
   "outputs": [],
   "source": [
    "from warnings import filterwarnings \n",
    "filterwarnings('ignore')\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy import stats\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.metrics import r2_score\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.stats import zscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-30T03:45:53.797744Z",
     "start_time": "2023-08-30T03:45:52.930785Z"
    }
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/vehicles.csv')\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isna().sum().sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.eq(0).sum().sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_values = data.nunique().sort_values()\n",
    "unique_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pd.Series({col:data[col].unique() for col in data})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(ncols=2)\n",
    "for i, yvar in enumerate(['odometer', 'year']):\n",
    "    axes[i].scatter(data['price'],data[yvar])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.pairplot(data,x_vars=['year','odometer'], y_vars=['price'],diag_kind=\"kde\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.displot(data, x=\"year\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(data, x=\"fuel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(data, x=\"paint_color\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(data, x=\"title_status\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation\n",
    "\n",
    "After our initial exploration and fine tuning of the business understanding, it is time to construct our final dataset prior to modeling.  Here, we want to make sure to handle any integrity issues and cleaning, the engineering of new features, any transformations that we believe should happen (scaling, logarithms, normalization, etc.), and general preparation for modeling with `sklearn`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove dups as VIN will be unique, keeping odometer highest as that will be latest data point\n",
    "data = data.sort_values('odometer', ascending=True).drop_duplicates('VIN', keep='last').sort_index()\n",
    "#dropping id column as it is useless in identifying price\n",
    "data = data.drop(['id'], axis=1)\n",
    "\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encode some of the features which makes sense..\n",
    "onehotencoded_columns = ['manufacturer']\n",
    "# Assign dataframe using pd.get_dummies to encode categorical values into separate columns\n",
    "data = pd.get_dummies(data, columns=onehotencoded_columns, dtype='int', drop_first=False)\n",
    "\n",
    "data.update(data[['region', 'condition', 'state','model','cylinders','fuel','title_status','transmission','drive','size','type','paint_color']].apply(lambda s: s.map(data['price'].groupby(s).mean())))\n",
    "\n",
    "finaldata = data.drop(['VIN'], axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find and fill null values\n",
    "finaldata.isna().sum()\n",
    "finaldata.fillna(method=\"ffill\",inplace=True)\n",
    "finaldata.fillna(method=\"bfill\",inplace=True)\n",
    "\n",
    "finaldata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove outliers with Z score\n",
    "finaldata = finaldata[(np.abs(zscore(finaldata)) <= 3).all(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = finaldata.corr().abs()\n",
    "\n",
    "values = (corr.where(np.triu(np.ones(corr.shape), k=1).astype(bool)).stack().sort_values(ascending=False))\n",
    "for index, value in enumerate(values.items()):\n",
    "    if index == 25:\n",
    "        break\n",
    "    print(index, value)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dfCorr = finaldata.corr()\n",
    "# filteredDf = dfCorr[((dfCorr >= .5) | (dfCorr <= -.5)) & (dfCorr !=1.000)]\n",
    "# plt.figure(figsize=(15,10))\n",
    "# sns.heatmap(finaldata, annot=True, cmap=\"coolwarm\")\n",
    "# plt.show()\n",
    "\n",
    "# Split data\n",
    "\n",
    "X = finaldata.drop('price', axis=1)\n",
    "y = finaldata['price']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RFE selection\n",
    "\n",
    "\n",
    "model = LinearRegression()\n",
    "# Create an RFE selector \n",
    "recursive_feature_elimination = RFE(model, n_features_to_select=5)\n",
    "\n",
    "# Fit the selector to the data\n",
    "recursive_feature_elimination.fit(X_train, y_train)\n",
    "\n",
    "# selected features\n",
    "selected_features = X_train.columns[recursive_feature_elimination.support_]\n",
    "\n",
    "print(selected_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PCA model\n",
    "# Scale numerical features\n",
    "pca_columns = finaldata.columns\n",
    "scaler = StandardScaler()\n",
    "scaled_numerical = scaler.fit_transform(X_train)\n",
    "\n",
    "# Apply PCA on scaled numerical features\n",
    "pca = PCA()\n",
    "transformed_numerical = pca.fit_transform(scaled_numerical)\n",
    "# Bar graph for explained variance ratios\n",
    "explained_variance_ratio = pca.explained_variance_ratio_\n",
    "plt.bar(range(1, len(explained_variance_ratio) + 1), explained_variance_ratio)\n",
    "plt.xlabel('PCA Components')\n",
    "plt.ylabel('Explained Variance Ratio')\n",
    "plt.title('Explained Variance Ratio by PCA Components')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(transformed_numerical[:, 0], transformed_numerical[:, 1])\n",
    "plt.xlabel(\"PCA Component 1\")\n",
    "plt.ylabel(\"PCA 2\")\n",
    "plt.title(\"PCA Analysis\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loadings = pca.components_\n",
    "num_pc = pca.n_features_\n",
    "pc_list = [\"PC\"+str(i) for i in list(range(1, num_pc+1))]\n",
    "loadings_df = pd.DataFrame.from_dict(dict(zip(pc_list, loadings)))\n",
    "finaldata.columns.values\n",
    "loadings_df['variable'] = X_train.columns.values\n",
    "loadings_df = loadings_df.set_index('variable')\n",
    "loadings_df\n",
    "loadings_df.corr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling\n",
    "\n",
    "With your (almost?) final dataset in hand, it is now time to build some models.  Here, you should build a number of different regression models with the price as the target.  In building your models, you should explore different parameters and be sure to cross-validate your findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Create a column transformer to apply one-hot to selected variables\n",
    "col_transformer = make_column_transformer(\n",
    "        (OneHotEncoder(drop='if_binary', handle_unknown='ignore'), X_train.columns.tolist()),\n",
    "        remainder='passthrough'\n",
    ")\n",
    "\n",
    "# Fit and transform the column transformer with training data\n",
    "X_train_transformed = col_transformer.fit_transform(X_train)\n",
    "\n",
    "# Create a new column transformer for test data\n",
    "col_transformer_test = make_column_transformer(\n",
    "        (OneHotEncoder(drop='if_binary', handle_unknown='ignore'), X_train.columns.tolist()),\n",
    "        remainder='passthrough'\n",
    ")\n",
    "\n",
    "# Fit the column transformer with training data (it already transformed X_train)\n",
    "col_transformer_test.fit(X_train)\n",
    "\n",
    "# Transform X_test using the col_transformer_test\n",
    "X_test_transformed = col_transformer_test.transform(X_test)\n",
    "\n",
    "# Create a Pipeline for data processing with linear regression\n",
    "pipe = Pipeline([\n",
    "        ('linreg', LinearRegression())\n",
    "])\n",
    "\n",
    "# Train the pipeline on the transformed training data\n",
    "pipe.fit(X_train_transformed, y_train)\n",
    "\n",
    "# Predict y from the transformed test dataset\n",
    "y_test_pred = pipe.predict(X_test_transformed)\n",
    "\n",
    "# Predict y from our test dataset\n",
    "y_train_pred = pipe.predict(X_train_transformed)\n",
    "\n",
    "# Calculate mean squared error\n",
    "mse = mean_squared_error(y_test, y_test_pred)\n",
    "print(\"Y Test Mean Squared Error:\", mse)\n",
    "# Calculate mean squared error\n",
    "mse = mean_squared_error(y_train, y_train_pred)\n",
    "print(\"Y Train Mean Squared Error:\", mse)\n",
    "\n",
    "# Calculate R-squared (Coefficient of Determination)\n",
    "r2 = r2_score(y_test, y_test_pred)\n",
    "print(f\"R-squared (Coefficient of Determination): {r2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate residuals\n",
    "residuals = y_test - y_test_pred\n",
    "\n",
    "# Create a DataFrame with residuals\n",
    "residuals_df = pd.DataFrame({'Actual': y_test, 'Predicted': y_test_pred, 'Residuals': residuals})\n",
    "\n",
    "# Plot residuals\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(residuals_df['Predicted'], residuals_df['Residuals'], alpha=0.5)\n",
    "plt.axhline(y=0, color='r', linestyle='--')\n",
    "plt.title('Residuals Plot')\n",
    "plt.xlabel('Predicted Values')\n",
    "plt.ylabel('Residuals')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Create a Pipeline for data processing with linear regression\n",
    "pipe = Pipeline([\n",
    "    ('poly', PolynomialFeatures(degree=1)),\n",
    "    ('linreg', LinearRegression())\n",
    "])\n",
    "\n",
    "# Train the pipeline on the transformed training data\n",
    "pipe.fit(X_train_transformed, y_train)\n",
    "\n",
    "# Predict y from our test dataset\n",
    "y_train_pred = pipe.predict(X_train_transformed)\n",
    "\n",
    "# Predict y from the transformed test dataset\n",
    "y_test_pred = pipe.predict(X_test_transformed)\n",
    "\n",
    "\n",
    "# Calculate mean squared error\n",
    "mse = mean_squared_error(y_test, y_test_pred)\n",
    "print(\"Y Test Mean Squared Error:\", mse)\n",
    "# Calculate mean squared error\n",
    "mse = mean_squared_error(y_train, y_train_pred)\n",
    "print(\"Y Train Mean Squared Error:\", mse)\n",
    "\n",
    "# Calculate R-squared (Coefficient of Determination)\n",
    "r2 = r2_score(y_test, y_test_pred)\n",
    "print(f\"R-squared (Coefficient of Determination): {r2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create a LinearRegression model\n",
    "linreg = LinearRegression()\n",
    "\n",
    "# Train the model on the training data\n",
    "linreg.fit(X_train, y_train)\n",
    "\n",
    "# Predict y from the test dataset\n",
    "y_test_pred = linreg.predict(X_test)\n",
    "\n",
    "# Predict y from the training dataset\n",
    "y_train_pred = linreg.predict(X_train)\n",
    "\n",
    "# Calculate mean squared error for test data\n",
    "mse_test = mean_squared_error(y_test, y_test_pred)\n",
    "print(\"Y Test Mean Squared Error:\", mse_test)\n",
    "\n",
    "# Calculate mean squared error for train data\n",
    "mse_train = mean_squared_error(y_train, y_train_pred)\n",
    "print(\"Y Train Mean Squared Error:\", mse_train)\n",
    "\n",
    "# Calculate R-squared (Coefficient of Determination)\n",
    "r2 = r2_score(y_test, y_test_pred)\n",
    "print(f\"R-squared (Coefficient of Determination): {r2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "With some modeling accomplished, we aim to reflect on what we identify as a high quality model and what we are able to learn from this.  We should review our business objective and explore how well we can provide meaningful insight on drivers of used car prices.  Your goal now is to distill your findings and determine whether the earlier phases need revisitation and adjustment or if you have information of value to bring back to your client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "\n",
    "\n",
    "x_cat_columns = X_train.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "# Create a column transformer to apply one-hot to selected variables\n",
    "col_transformer = make_column_transformer(\n",
    "    (OneHotEncoder(drop='if_binary', handle_unknown='ignore'), x_cat_columns),\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "# Fit and transform the column transformer with training data\n",
    "X_train_transformed = col_transformer.fit_transform(X_train)\n",
    "\n",
    "# Transform X_test using the col_transformer\n",
    "X_test_transformed = col_transformer.transform(X_test)\n",
    "\n",
    "# Define the parameter grid for grid search\n",
    "param_grid = {\n",
    "    'linreg__fit_intercept': [True, False]\n",
    "}\n",
    "\n",
    "# Create a Pipeline for data processing with linear regression\n",
    "pipe = Pipeline([\n",
    "    ('linreg', LinearRegression())\n",
    "])\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search = GridSearchCV(pipe, param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "\n",
    "# Train models using grid search\n",
    "grid_search.fit(X_train_transformed, y_train)\n",
    "\n",
    "# Get the best estimator from grid search\n",
    "best_pipe = grid_search.best_estimator_\n",
    "\n",
    "# Predict y from the transformed test dataset using the best model\n",
    "y_test_pred = best_pipe.predict(X_test_transformed)\n",
    "\n",
    "# Predict y from the training dataset using the best model\n",
    "y_train_pred = best_pipe.predict(X_train_transformed)\n",
    "\n",
    "# Calculate mean squared error for test data\n",
    "mse_test = mean_squared_error(y_test, y_test_pred)\n",
    "print(\"Y Test Mean Squared Error:\", mse_test)\n",
    "\n",
    "# Calculate mean squared error for train data\n",
    "mse_train = mean_squared_error(y_train, y_train_pred)\n",
    "print(\"Y Train Mean Squared Error:\", mse_train)\n",
    "\n",
    "# Calculate R-squared (Coefficient of Determination)\n",
    "r2 = r2_score(y_test, y_test_pred)\n",
    "print(f\"R-squared (Coefficient of Determination): {r2}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Add polynomial features\n",
    "degree = 2  # Change the degree as needed\n",
    "poly = PolynomialFeatures(degree=degree)\n",
    "X_train_poly = poly.fit_transform(X_train)\n",
    "X_test_poly = poly.transform(X_test)\n",
    "\n",
    "# Create a LinearRegression model\n",
    "linreg = LinearRegression()\n",
    "\n",
    "# Hyperparameter tuning using GridSearch\n",
    "param_grid = {\n",
    "    'fit_intercept': [True, False],\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(estimator=linreg, param_grid=param_grid, cv=5)\n",
    "grid_search.fit(X_train_poly, y_train)\n",
    "\n",
    "best_linreg = grid_search.best_estimator_\n",
    "\n",
    "# Train the best model on the training data\n",
    "best_linreg.fit(X_train_poly, y_train)\n",
    "\n",
    "# Predict y from the test dataset using the best model\n",
    "y_test_pred_best = best_linreg.predict(X_test_poly)\n",
    "\n",
    "# Predict y from the training dataset using the best model\n",
    "y_train_pred_best = best_linreg.predict(X_train_poly)\n",
    "\n",
    "# Calculate mean squared error for test data using the best model\n",
    "mse_test_best = mean_squared_error(y_test, y_test_pred_best)\n",
    "print(\"Best Model Y Test Mean Squared Error:\", mse_test_best)\n",
    "\n",
    "# Calculate mean squared error for train data using the best model\n",
    "mse_train_best = mean_squared_error(y_train, y_train_pred_best)\n",
    "print(\"Best Model Y Train Mean Squared Error:\", mse_train_best)\n",
    "\n",
    "# Calculate R-squared (Coefficient of Determination) for test data using the best model\n",
    "r2_best = r2_score(y_test, y_test_pred_best)\n",
    "print(f\"Best Model R-squared (Coefficient of Determination): {r2_best}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of folds\n",
    "n_folds = 5\n",
    "\n",
    "# Create regression models\n",
    "linear_reg_model = LinearRegression()\n",
    "\n",
    "# Define the mean squared error as the scoring metric\n",
    "scorer = make_scorer(mean_squared_error)\n",
    "\n",
    "# Perform K-Fold Cross-Validation for Linear Regression\n",
    "linear_reg_scores = cross_val_score(linear_reg_model, X, y, cv=n_folds, scoring=scorer)\n",
    "linear_reg_mse = linear_reg_scores.mean()\n",
    "\n",
    "\n",
    "\n",
    "# Print the mean squared error for each model\n",
    "print(\"Linear Regression Cross validation X1-Mean Squared Error:\", linear_reg_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of folds\n",
    "n_folds = 5\n",
    "\n",
    "# Create a Random Forest Regressor model\n",
    "lg = LinearRegression()\n",
    "\n",
    "# Fit the model on your training data\n",
    "lg.fit(X, y)\n",
    "\n",
    "# Perform cross-validation to get mean squared error scores\n",
    "mse_scores = cross_val_score(lg, X, y, cv=n_folds, scoring='neg_mean_squared_error')\n",
    "\n",
    "# Convert negative scores to positive (mean squared error)\n",
    "mse_scores = -mse_scores\n",
    "\n",
    "# Calculate the average mean squared error\n",
    "average_mse = mse_scores.mean()\n",
    "\n",
    "# Print the mean squared error for each fold\n",
    "for i, mse in enumerate(mse_scores):\n",
    "    print(f\"Fold {i+1} Mean Squared Error: {mse}\")\n",
    "\n",
    "# Print the average mean squared error\n",
    "print(\"Average Mean Squared Error:\", average_mse)\n",
    "\n",
    "# Perform cross-validation to get predicted target values\n",
    "predicted_y = cross_val_predict(lg, X, y, cv=n_folds)\n",
    "\n",
    "# Calculate R-squared\n",
    "r_squared = lg.score(X, y)\n",
    "\n",
    "# Print R-squared\n",
    "print(\"R-squared (Coefficient of Determination):\", r_squared)\n",
    "\n",
    "# Calculate Mean Squared Error between predicted and actual target values\n",
    "mse = mean_squared_error(y, predicted_y)\n",
    "print(\"Overall Mean Squared Error:\", mse)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deployment\n",
    "\n",
    "Now that we've settled on our models and findings, it is time to deliver the information to the client.  You should organize your work as a basic report that details your primary findings.  Keep in mind that your audience is a group of used car dealers interested in fine tuning their inventory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
